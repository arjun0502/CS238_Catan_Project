{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catanatron import Game, Player, RandomPlayer, Color\n",
    "from catanatron_gym.envs.catanatron_env import from_action_space, to_action_space\n",
    "from catanatron_gym.features import create_sample_vector\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, dim_state, num_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.lin1 = nn.Linear(dim_state, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        return self.lin2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill these in\n",
    "def reward(state, action):\n",
    "    ### TODO #######################\n",
    "    return 0\n",
    "\n",
    "def state_tensor(state):\n",
    "    #### TODO #######################\n",
    "    return torch.tensor(state, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "POLICY_UPDATE = 100 # Number of actions before retraining\n",
    "TARGET_UPDATE = 5 # Number of episodes before updating target network\n",
    "NUM_EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"catanatron_gym:catanatron-v0\")\n",
    "state, info = env.reset()\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "dim_state = state_tensor(state).shape[1]\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.05\n",
    "# EPS_DECAY = 200\n",
    "# TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(dim_state, num_actions, HIDDEN_SIZE).to(device)\n",
    "target_net = DQN(dim_state, num_actions, HIDDEN_SIZE).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), LR)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    states = torch.cat(batch.state)\n",
    "    actions = torch.tensor(batch.action)\n",
    "    next_states = torch.cat([state for state in batch.next_state if state is not None])\n",
    "    rewards = torch.tensor(batch.reward)\n",
    "    dones = torch.tensor(batch.done)\n",
    "\n",
    "    cur_Q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    target_Q = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        target_Q[~dones] = target_net(next_states).max(1).values\n",
    "    target_Q = rewards + GAMMA * target_Q\n",
    "\n",
    "    # Update the policy network\n",
    "    loss = F.mse_loss(cur_Q, target_Q)\n",
    "    optimizer.zero_grad()\n",
    "    ### TODO: Clip Gradients #######################\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode():\n",
    "    state, info = env.reset()\n",
    "    state = state_tensor(state)\n",
    "    done = False\n",
    "    i = 0\n",
    "\n",
    "    while not done:\n",
    "        action = random.choice(env.get_valid_actions()) \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = state_tensor(next_state) if not done else None\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "        i += 1\n",
    "        if i % POLICY_UPDATE == 0:\n",
    "            train_batch()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for i in tqdm(range(NUM_EPISODES)):\n",
    "        train_episode()\n",
    "        if i % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_action(state, eps):\n",
    "#     if random.random() < eps:\n",
    "#         return torch.tensor([[random.randrange(num_actions)]], device=device, dtype=torch.long)\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             return policy_net(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:06<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPlayer(Player):\n",
    "    def __init__(self, model, color):\n",
    "        super().__init__(color)\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def decide(self, game, valid_actions):\n",
    "        state = np.array(create_sample_vector(game, self.color))\n",
    "        state = state_tensor(state)\n",
    "        action_ints = list(map(to_action_space, valid_actions))\n",
    "        with torch.no_grad():\n",
    "            Q = self.model(state)[0, action_ints]\n",
    "        best_action = action_ints[Q.max(0).indices.item()]\n",
    "        return from_action_space(best_action, valid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:41<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "players = [\n",
    "    RandomPlayer(Color.WHITE),\n",
    "    DQNPlayer(policy_net, Color.ORANGE),\n",
    "]\n",
    "\n",
    "winners = []\n",
    "for i in tqdm(range(100)):\n",
    "    game = Game(players)\n",
    "    winners.append(game.play())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winners.count(Color.ORANGE) / len(winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catanatron_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(catanatron_gym.features.create_sample_vector(game, Color.WHITE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
